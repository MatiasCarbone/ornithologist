{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ornithologist - CNN Bird Sounds Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import fnmatch\n",
    "import opendatasets as od\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa as li\n",
    "import soundfile as sf\n",
    "import scipy.signal as signal\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset\n",
    "Currently using the [British Birdsong Dataset](https://www.kaggle.com/datasets/rtatman/british-birdsong-dataset?resource=download).\n",
    "\n",
    "To download sound files from Kaggle, get the kaggle.json API token from [here](https://www.kaggle.com/settings) an place it in the notebooks folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = (\"https://www.kaggle.com/datasets/rtatman/british-birdsong-dataset?resource=download\")\n",
    "path_to_download = \"../datasets\"\n",
    "\n",
    "od.download(dataset_url, data_dir=path_to_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = path_to_download + '/british-birdsong-dataset/birdsong_metadata.csv'\n",
    "\n",
    "metadata = pd.read_csv(csv_path)[['file_id', 'english_cname']]\n",
    "metadata.rename(columns={'english_cname':'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio processing function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- butter_highpass() generates the coefficients for a butterworth high pass filter\n",
    "- apply_butter_highpass() takes the filter coefficients and apply the filter to a signal\n",
    "- remove_silence() takes a signal and removes the sections that fall below a certain threshold\n",
    "- split_audio() takes a long audio and splits it into segments of equal length, zero-padding the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def apply_butter_highpass(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence(signal, thresh=20, hop=2048, plot=False):\n",
    "  splits = li.effects.split(y=signal, top_db=thresh, frame_length=(hop * 4), hop_length=hop)\n",
    "  \n",
    "  if plot:\n",
    "    peak = np.max(signal)\n",
    "    plt.subplots(figsize=(12,4))\n",
    "    plt.plot(signal)\n",
    "    plt.vlines(splits, ymin=-peak, ymax=peak, color='red')\n",
    "    plt.show()\n",
    "  \n",
    "  stripped_audio = []\n",
    "  \n",
    "  for s in splits:\n",
    "    split = signal[s[0]:s[1]]\n",
    "    stripped_audio.extend(split)\n",
    "  \n",
    "  return np.asarray(stripped_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(signal, target_length, samplerate=22050, plot=False): # Target length must be in seconds\n",
    "  duration = li.get_duration(y=signal, sr=samplerate)\n",
    "  n_segments = np.ceil(duration / target_length)\n",
    "   \n",
    "  audio_segments = []\n",
    "  \n",
    "  for n in range(int(n_segments)):\n",
    "    s = signal[samplerate * n * target_length : samplerate * (n + 1) * target_length]\n",
    "    \n",
    "    if len(s) < target_length * samplerate:\n",
    "      s = np.pad(s, (0, target_length * samplerate - len(s)), 'constant')\n",
    "    \n",
    "    audio_segments.append(s)\n",
    "    \n",
    "    if plot:\n",
    "      plt.plot(s, alpha=1/n_segments)\n",
    "    \n",
    "  if plot:\n",
    "    plt.show()\n",
    "\n",
    "  return audio_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing files\n",
    "In this section, the files listed in the metadata csv are loaded, pre-processed and saved to disk as multiple labeled samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined constants\n",
    "AUDIO_LENGTH = 3 # Seconds\n",
    "TARGET_SAMPLE_RATE = 22050\n",
    "\n",
    "sound_files_path = path_to_download + '/british-birdsong-dataset/songs/songs/'\n",
    "\n",
    "# Create empty folder processes files folder if not exists\n",
    "save_path = sound_files_path + 'processed_samples/'\n",
    "if os.path.exists(save_path):\n",
    "  shutil.rmtree(save_path)\n",
    "  os.mkdir(save_path)\n",
    "else: \n",
    "  os.mkdir(save_path)\n",
    "\n",
    "for index, file_id, label in metadata.itertuples():\n",
    "  # Read audio file from disk\n",
    "  filename = sound_files_path + 'xc' + str(file_id) + '.flac'\n",
    "  y, sr = li.load(filename, sr=TARGET_SAMPLE_RATE, mono=True)\n",
    "    \n",
    "  # Apply high-pass filter\n",
    "  y = apply_butter_highpass(data=y, cutoff=800, fs=sr, order=6)\n",
    "  \n",
    "  # Delete silent sections\n",
    "  y = remove_silence(y, plot=False)\n",
    "  \n",
    "  # Split into segments of desired length\n",
    "  audio_segments = split_audio(y, target_length=AUDIO_LENGTH, samplerate=sr)\n",
    "  \n",
    "  # Apply data augmentation (optional)\n",
    "    \n",
    "  # Save samples to new folder on disk, with incremental filenames\n",
    "  for segment in audio_segments:\n",
    "    count = len(fnmatch.filter(os.listdir(save_path), label + '*.wav'))\n",
    "    sf.write(f'{save_path}{label}_{count + 1}.wav', segment, sr, format='wav', subtype='PCM_16') # Changed from flac to wav while testing\n",
    "    \n",
    "n_samples = len(os.listdir(save_path))\n",
    "print(f'Done. Saved {n_samples} audio files of {AUDIO_LENGTH} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create categories dictionary for label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels_list):\n",
    "    unique_labels_dict = {}\n",
    "\n",
    "    for i, label in enumerate(set(labels_list)):\n",
    "        unique_labels_dict.update({label: i})\n",
    "\n",
    "    return unique_labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_DICT = encode_labels(metadata['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get label from tensor containing file path, without encoding\n",
    "def get_label(path_tensor):\n",
    "  filename = tf.strings.split(path_tensor, os.path.sep)[-1]\n",
    "  label = tf.strings.split(filename, '.')[0]\n",
    "  label = tf.strings.split(label, '_')[0]\n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process an audio from file path tensor.\n",
    "# Returns a tensor containing mel-spectrogram and label.\n",
    "def process_audio(filename_tensor, plot=False):\n",
    "  label = get_label(filename_tensor)\n",
    "  sr = TARGET_SAMPLE_RATE\n",
    "  \n",
    "  # Load and reshape audio\n",
    "  raw_file = tf.io.read_file(filename_tensor)\n",
    "  audio, _ = tf.audio.decode_wav(raw_file)\n",
    "  audio = tf.squeeze(audio, axis=[-1])\n",
    "  \n",
    "  # Apply short fade-in and fade-out\n",
    "  audio = tfio.audio.fade(audio, fade_in=200, fade_out=200, mode=\"linear\")\n",
    "  \n",
    "  # Optional wave plot\n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.plot(audio.numpy())\n",
    "    \n",
    "  # Generate mel-spectrogram\n",
    "  spectrogram = tfio.audio.spectrogram(audio, nfft=512, window=512, stride=256)\n",
    "  mel_spectrogram = tfio.audio.melscale(spectrogram, rate=sr, mels=128, fmin=0, fmax=int(sr/2))\n",
    "  \n",
    "  # Optional spectrogram plot\n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(tf.math.log(spectrogram).numpy())\n",
    "    \n",
    "  return mel_spectrogram, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_ds = tf.data.Dataset.list_files(save_path + '*.wav', shuffle=True)\n",
    "\n",
    "# Train/test splitting\n",
    "n_samples = len(audio_ds)\n",
    "train_split = int(n_samples * 0.8)\n",
    "train_ds = audio_ds.take(train_split)\n",
    "test_ds = audio_ds.skip(train_split)\n",
    "\n",
    "print(f'Train dataset: {len(train_ds)} files.\\nTest dataset: {len(test_ds)} files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply processing function and batching to datasets\n",
    "train_ds = train_ds.map(process_audio)#.batch(32, drop_remainder=True)\n",
    "test_ds = test_ds.map(process_audio)#.batch(32, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a simple CNN with 2x Conv+Maxpool + 1 Dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_class_count = len(metadata['label'].unique())\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='same',\n",
    "                           activation='relu', input_shape=(259, 128)),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='same',\n",
    "                           activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    tf.keras.layers.Dense(units=unique_class_count, activation='softmax')\n",
    "])\n",
    "\n",
    "model.build()\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with data from tf.data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "history = model.fit(x=train_ds, batch_size=10, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of original audio file durations\n",
    "plt.subplots(figsize=(12, 4))\n",
    "plt.title('Distribution of Audio File Lenght')\n",
    "plt.xlabel('Time in seconds')\n",
    "plt.ylabel('Number of files')\n",
    "plt.xticks(range(0, 300, 10))\n",
    "plt.hist(metadata['duration'], bins=100)\n",
    "plt.show()\n",
    "\n",
    "print(f'• Original file count: {len(metadata)}')\n",
    "# print(f'• Total file count after splitting: {len(audio_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot total duration for each label\n",
    "duration_sum = metadata.groupby('label').sum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(24, 8))\n",
    "ax.set_title('Total raw duration for each label')\n",
    "ax.bar(x=duration_sum.index, height=duration_sum['duration'])\n",
    "ax.set_ylabel('Duration (in seconds)')\n",
    "ax.set_xlabel('Labels')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
