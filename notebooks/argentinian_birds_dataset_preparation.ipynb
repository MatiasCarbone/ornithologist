{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import opendatasets as od\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import librosa as li\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate butterworth highpass coefficients\n",
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "\n",
    "\n",
    "# Apply filter to signal\n",
    "def apply_butter_highpass(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Remove sections of silence or low intensity signal\n",
    "def remove_silence(signal, thresh=18, hop=2048, plot=False):\n",
    "    splits = li.effects.split(y=signal, top_db=thresh, frame_length=(hop * 2), hop_length=hop)\n",
    "    if plot:\n",
    "        peak = np.max(signal)\n",
    "        plt.subplots(figsize=(12, 4))\n",
    "        plt.plot(signal)\n",
    "        plt.vlines(splits, ymin=-peak, ymax=peak, color='red')\n",
    "        plt.show()\n",
    "\n",
    "    stripped_audio = []\n",
    "\n",
    "    for s in splits:\n",
    "        split = signal[s[0] : s[1]]\n",
    "        stripped_audio.extend(split)\n",
    "\n",
    "    return np.asarray(stripped_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Split audio into segments of desired length\n",
    "def split_audio(signal, target_length, samplerate, plot=False):\n",
    "    duration = li.get_duration(y=signal, sr=samplerate)\n",
    "    n_segments = np.ceil(duration / target_length)\n",
    "    audio_segments = []\n",
    "\n",
    "    for n in range(int(n_segments)):\n",
    "        s = signal[samplerate * n * target_length : samplerate * (n + 1) * target_length]\n",
    "\n",
    "        if len(s) < target_length * samplerate:\n",
    "            s = np.pad(s, (0, target_length * samplerate - len(s)), 'constant')\n",
    "\n",
    "        audio_segments.append(s)\n",
    "\n",
    "        if plot:\n",
    "            plt.plot(s, alpha=1 / n_segments)\n",
    "\n",
    "    if plot:\n",
    "        plt.show()\n",
    "\n",
    "    return audio_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Apply all signal processing functions to audio and return segments\n",
    "def generate_preprocessed_samples(path, sr, length, hp=700):\n",
    "\n",
    "    y, sr = li.load(path, sr=sr, mono=True)  # Load audio file\n",
    "\n",
    "    y = apply_butter_highpass(data=y, cutoff=hp, fs=sr, order=5)  # Apply high-pass filter\n",
    "\n",
    "    # Delete silent sections\n",
    "    y = remove_silence(y, thresh=18, hop=2048, plot=False)\n",
    "\n",
    "    audio_segments = split_audio(\n",
    "        y, target_length=length, samplerate=sr\n",
    "    )  # Split into segments of desired length\n",
    "\n",
    "    return audio_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio dataset download and preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "MIN_RECORDED_TIME = 100\n",
    "MAX_RECORDED_TIME = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database query and metadata generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset_location = '..\\\\datasets\\\\xeno-canto_argentina\\\\'\n",
    "\n",
    "# Query filters\n",
    "country = 'argentina'\n",
    "since = '1980-01-01'\n",
    "group = 'birds'\n",
    "length = '10-60'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "api_url = 'https://xeno-canto.org/api/2/recordings?query='\n",
    "params = f\"cnt:{country}+grp:{group}+len:{length}+since:{since}\"\n",
    "\n",
    "response = requests.get(api_url + params)\n",
    "\n",
    "print(f\"• Query result: status-code {response.status_code}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "    n_rec = data[\"numRecordings\"]\n",
    "    pages = data[\"numPages\"]\n",
    "    print(f\"• Found {n_rec} recordings in {pages} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset folder\n",
    "try:\n",
    "    os.mkdir(dataset_location)\n",
    "    print('Created new folder: ' + dataset_location)\n",
    "except:\n",
    "    try:\n",
    "        os.mkdir('..\\\\datasets')\n",
    "        os.mkdir(dataset_location)\n",
    "    except:\n",
    "        print(dataset_location + ' already exists\\n')\n",
    "\n",
    "# Store each page of the request into a Pandas dataframe\n",
    "df_list = []\n",
    "for page in range(1, pages + 1):\n",
    "    print(f'Working on page {page}...', end=\"   \")\n",
    "    response = requests.get(api_url + params + f\"&page={page}\")\n",
    "    data = response.json()\n",
    "    df = pd.json_normalize(data['recordings'])\n",
    "    df_list.append(df)\n",
    "    print('Done!')\n",
    "\n",
    "# Concatenate all dataframes into one and save to disk\n",
    "recordings_dataframe = pd.concat(df_list, ignore_index=True)\n",
    "recordings_dataframe.to_csv(dataset_location + 'metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter metadata entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    dataset_location + 'metadata.csv',\n",
    "    usecols=[\n",
    "        'id',\n",
    "        'group',\n",
    "        'gen',\n",
    "        'sp',\n",
    "        'ssp',\n",
    "        'en',\n",
    "        'loc',\n",
    "        'type',\n",
    "        'file',\n",
    "        'q',\n",
    "        'length',\n",
    "        'method',\n",
    "        'file-name',\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Filter metadata\n",
    "df = df.loc[\n",
    "    (df['group'] == 'birds')\n",
    "    & (df['gen'] != 'Mystery')\n",
    "    & (df['sp'] != 'mystery')\n",
    "    & (df['q'].isin(['A']))\n",
    "    & (df['method'] == 'field recording')\n",
    "    & (df['type'].isin(['call', 'song']))\n",
    "]\n",
    "\n",
    "# Convert length column from minutes to seconds for aggregation\n",
    "df['length'] = df['length'].map(minutes_to_seconds)\n",
    "\n",
    "# Get total audio length for each species and filter by total length\n",
    "df_group = df.groupby(['gen', 'sp'], as_index=False).sum('length')\n",
    "df_group = df_group.loc[df_group['length'] > MIN_RECORDED_TIME]\n",
    "\n",
    "# Get filtered dataframe\n",
    "species_filter = (df_group['gen'] + ' ' + df_group['sp']).values.tolist()\n",
    "df_filter = df.loc[(df['gen'] + ' ' + df['sp']).isin(species_filter)]\n",
    "\n",
    "print(\n",
    "    f'{len(species_filter)} distinct species with at least {MIN_RECORDED_TIME} seconds '\n",
    "    + f'of recording were selected, with a total of {len(df_filter)} audio files.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_group.sort_values('length', ascending=False).iloc[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download audio data from Xeno-Canto database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create subfolders for each species\n",
    "audio_location = dataset_location + 'audio\\\\'\n",
    "\n",
    "os.mkdir(audio_location)\n",
    "for sp in species_filter:\n",
    "    os.mkdir(os.path.join(audio_location, sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Store downloaded time in a dictionary\n",
    "downloaded_time_per_species = dict.fromkeys(species_filter, 0)\n",
    "\n",
    "for index in df_filter.index:\n",
    "    # Get data from df row\n",
    "    species = df['gen'][index] + ' ' + df['sp'][index]\n",
    "    id = df['id'][index]\n",
    "    length = df['length'][index]\n",
    "    url = df['file'][index]\n",
    "    ext = df['file-name'][index].split('.')[-1]\n",
    "\n",
    "    # Download files for each species until max_recorded_time is reached\n",
    "    if (downloaded_time_per_species[species] + length) <= MAX_RECORDED_TIME:\n",
    "        downloaded_time_per_species[species] += length\n",
    "        \n",
    "        filename = f'{species}_{id}.{ext}'\n",
    "\n",
    "        with open(os.path.join(audio_location, species, filename), 'wb') as out_file:\n",
    "            content = requests.get(url, stream=True).content\n",
    "            out_file.write(content)\n",
    "        \n",
    "        # Wait required time between recordings\n",
    "        sleep(1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio preprocessing and feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Target sample rate for resampling audio files\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# Target length for audio segments (in seconds)\n",
    "SAMPLE_LENGTH = 3\n",
    "\n",
    "dataset_path = '..\\\\datasets\\\\xeno-canto_argentina\\\\'\n",
    "audio_path = '..\\\\datasets\\\\xeno-canto_argentina\\\\audio\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: fix function so it does not load all MFCCs into memory\n",
    "\n",
    "def preprocess_audio_dataset(data_path, json_path=None, mfcc_count=13, hop=512, fft_len=2048):\n",
    "data_dict = {\n",
    "'label_map': [],\n",
    "'encoded_labels': [],\n",
    "'mfccs': [],\n",
    "'files': [],\n",
    "}\n",
    "\n",
    "    for i, (path, _, files) in enumerate(os.walk(data_path)):\n",
    "        if path == data_path:  # Ignore parent folder\n",
    "            continue\n",
    "\n",
    "        # Add unique labels to label_map list\n",
    "        label = path.split('\\\\')[-1]\n",
    "        if label not in data_dict['label_map']:\n",
    "            data_dict['label_map'].append(label)\n",
    "\n",
    "        for f in files:\n",
    "            segments = generate_preprocessed_samples(os.path.join(path, f), sr=SAMPLE_RATE, length=SAMPLE_LENGTH)\n",
    "            for segment in segments:\n",
    "                # Add encoded label to encoded_labels list\n",
    "                index = data_dict['label_map'].index(label)\n",
    "                data_dict['encoded_labels'].append(index)\n",
    "\n",
    "                # Add original file path to files list\n",
    "                data_dict['files'].append(os.path.join(path, f))\n",
    "\n",
    "                # Load audio and compute MFCCs\n",
    "                y, sr = li.load(os.path.join(path, f), sr=None, mono=True)\n",
    "                mfccs = li.feature.mfcc(y=y, sr=sr, n_mfcc=mfcc_count, hop_length=hop, n_fft=fft_len)\n",
    "\n",
    "                # Append MFCCs to list. Casting np.array to list for saving as JSON file.\n",
    "                data_dict['mfccs'].append(mfccs.transpose().tolist())\n",
    "\n",
    "    # Store data dictionary in JSON file\n",
    "    if json_path:\n",
    "        with open(json_path, 'w') as jf:\n",
    "            json.dump(data_dict, jf, indent=4)\n",
    "            print(f'Successfully saved preprocessed data to {json_path}!')\n",
    "            file_count = len(data_dict['files'])\n",
    "            print(f'{file_count} audio samples were processed!')\n",
    "\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "json_path = dataset_path + 'preprocessed_data.json'\n",
    "preprocess_audio_dataset(audio_path, json_path=json_path, mfcc_count=13)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
