{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import opendatasets as od\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import librosa as li\n",
    "import soundfile as sf\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate butterworth highpass coefficients\n",
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "\n",
    "\n",
    "# Apply filter to signal\n",
    "def apply_butter_highpass(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove sections of silence or low intensity signal\n",
    "def remove_silence(signal, thresh=18, hop=2048, plot=False):\n",
    "    splits = li.effects.split(\n",
    "        y=signal, top_db=thresh, frame_length=(hop * 2), hop_length=hop\n",
    "    )\n",
    "    if plot:\n",
    "        peak = np.max(signal)\n",
    "        plt.subplots(figsize=(12, 4))\n",
    "        plt.plot(signal)\n",
    "        plt.vlines(splits, ymin=-peak, ymax=peak, color='red')\n",
    "        plt.show()\n",
    "\n",
    "    stripped_audio = []\n",
    "\n",
    "    for s in splits:\n",
    "        split = signal[s[0]: s[1]]\n",
    "        stripped_audio.extend(split)\n",
    "\n",
    "    return np.asarray(stripped_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split audio into segments of desired length\n",
    "def split_audio(signal, target_length, samplerate, plot=False):\n",
    "    duration = li.get_duration(y=signal, sr=samplerate)\n",
    "    n_segments = np.ceil(duration / target_length)\n",
    "    audio_segments = []\n",
    "\n",
    "    for n in range(int(n_segments)):\n",
    "        s = signal[\n",
    "            samplerate * n * target_length: samplerate * (n + 1) * target_length\n",
    "        ]\n",
    "\n",
    "        if len(s) < target_length * samplerate:\n",
    "            s = np.pad(s, (0, target_length * samplerate - len(s)), 'constant')\n",
    "\n",
    "        audio_segments.append(s)\n",
    "\n",
    "        if plot:\n",
    "            plt.plot(s, alpha=1 / n_segments)\n",
    "\n",
    "    if plot:\n",
    "        plt.show()\n",
    "\n",
    "    return audio_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all signal processing functions to audio and return segments\n",
    "def generate_preprocessed_samples(path, sr, length, hp=700):\n",
    "\n",
    "    y, sr = li.load(path, sr=sr, mono=True)  # Load audio file\n",
    "\n",
    "    y = apply_butter_highpass(\n",
    "        data=y, cutoff=hp, fs=sr, order=5\n",
    "    )  # Apply high-pass filter\n",
    "\n",
    "    # Delete silent sections\n",
    "    y = remove_silence(y, thresh=18, hop=2048, plot=False)\n",
    "\n",
    "    audio_segments = split_audio(\n",
    "        y, target_length=length, samplerate=sr\n",
    "    )  # Split into segments of desired length\n",
    "\n",
    "    return audio_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio dataset download and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and preparation of the [British Birdson Dataset](www.kaggle.com/datasets/rtatman/british-birdsong-dataset), a balanced and curated compilation of bird songs the Xeno-Canto database, consisting of 88 species from the UK. The collection was made by Dan Stowell and shared in Kaggle by Rachel Tatman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = (\n",
    "    'https://www.kaggle.com/datasets/rtatman/british-birdsong-dataset?resource=download'\n",
    ")\n",
    "dowload_path = '..\\\\datasets\\\\'\n",
    "\n",
    "od.download(dataset_url, data_dir=dowload_path)\n",
    "\n",
    "dataset_path = dowload_path + 'british-birdsong-dataset\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse metadata csv\n",
    "metadata = pd.read_csv(\n",
    "    dataset_path + 'birdsong_metadata.csv',\n",
    "    usecols=['file_id', 'genus', 'species', 'english_cname'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target sample rate for resampling audio files\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# Target length for audio segments (in seconds)\n",
    "SAMPLE_LENGTH = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all audio files from directory, apply filter, remove silent sections and split into segments. Save each segment to disk, with filename according to scientific bird name corresponding to dataset metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_path = dataset_path + 'songs\\\\songs\\\\'\n",
    "audio_path = dataset_path + 'audio\\\\'\n",
    "spec_path = dataset_path + 'spectrograms\\\\'\n",
    "\n",
    "try:\n",
    "    os.mkdir(audio_path)\n",
    "    print('Created new folder: ' + audio_path)\n",
    "except:\n",
    "    print(audio_path + ' already exists\\n')\n",
    "\n",
    "try:\n",
    "    os.mkdir(spec_path)\n",
    "    print('Created new folder: ' + spec_path)\n",
    "except:\n",
    "    print(spec_path + ' already exists\\n')\n",
    "\n",
    "for file in os.listdir(old_path):\n",
    "    id = file.lstrip('xc').rstrip('.flac')\n",
    "    data = metadata.loc[metadata['file_id'] == int(id)]\n",
    "\n",
    "    name = data['genus'].item() + '_' + data['species'].item()\n",
    "    subfolder = audio_path + name + '\\\\'\n",
    "    try:\n",
    "        os.mkdir(subfolder)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Preprocess audio and get segments of desired length\n",
    "    audio_segments = generate_preprocessed_samples(\n",
    "        old_path + file, sr=SAMPLE_RATE, length=SAMPLE_LENGTH\n",
    "    )\n",
    "\n",
    "    # Iterate through splitted audio segments and save each one as a separate flac file\n",
    "    for i, segment in enumerate(audio_segments):\n",
    "        new_filename = f'{subfolder}{name}_{id}_{i}.flac'\n",
    "        sf.write(new_filename, segment, SAMPLE_RATE, format='flac', subtype='PCM_16')\n",
    "\n",
    "shutil.rmtree(dataset_path + '\\\\songs')\n",
    "\n",
    "print('Finished generating audio samples!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset path\n",
    "dataset_path = '..\\\\datasets\\\\british-birdsong-dataset\\\\'\n",
    "# Get audio path\n",
    "audio_path = dataset_path + \"audio\\\\\"\n",
    "# Get spectrogram path\n",
    "spec_path = dataset_path + \"spectrograms\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read samples from disk and store file path, label, labels mapping and MFCC data into JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio_dataset(data_path, json_path=None, mfcc_count=13, hop=512, fft_len=2048):\n",
    "    data_dict = {\n",
    "        'label_map': [],\n",
    "        'encoded_labels': [],\n",
    "        'mfccs': [],\n",
    "        'files': [],\n",
    "        'spectrograms': [],\n",
    "    }\n",
    "\n",
    "    for i, (path, _, files) in enumerate(os.walk(data_path)):\n",
    "        if path == data_path:  # Ignore parent folder\n",
    "            continue\n",
    "\n",
    "        # Add unique labels to label_map list\n",
    "        label = path.split('\\\\')[-1]\n",
    "        if label not in data_dict['label_map']:\n",
    "            data_dict['label_map'].append(label)\n",
    "\n",
    "        # Create subfolder for spectrograms\n",
    "        os.mkdir(os.path.join(spec_path + label))\n",
    "\n",
    "        for f in files:\n",
    "            # Add encoded label to encoded_labels list\n",
    "            index = data_dict['label_map'].index(label)\n",
    "            data_dict['encoded_labels'].append(index)\n",
    "\n",
    "            # Add file path to files list\n",
    "            data_dict['files'].append(os.path.join(path, f))\n",
    "\n",
    "            # Load audio and add MFCCs to list\n",
    "            y, sr = li.load(os.path.join(path, f), sr=None, mono=True)\n",
    "            mfccs = li.feature.mfcc(y=y, sr=sr, n_mfcc=mfcc_count, hop_length=hop, n_fft=fft_len)\n",
    "\n",
    "            # Cast np.array to list is needed to save as JSON file\n",
    "            data_dict['mfccs'].append(mfccs.transpose().tolist())\n",
    "\n",
    "            # Generate mel spectrogram, add spec path to dict and save to NPY file\n",
    "            mel_spec = li.feature.melspectrogram(\n",
    "                y=y, sr=sr, n_fft=fft_len, hop_length=hop, power=0.5, fmin=200, n_mels=128\n",
    "            ).transpose()  # Transpose may not be needed!\n",
    "\n",
    "            npy_filename = f.replace('.flac', '.npy')\n",
    "            npy_filename = f'{spec_path}\\\\{label}\\\\{npy_filename}'\n",
    "            np.save(npy_filename, mel_spec)\n",
    "            data_dict['spectrograms'].append(npy_filename)\n",
    "\n",
    "    # Store data dictionary in JSON file\n",
    "    if json_path:\n",
    "        with open(json_path, 'w') as jf:\n",
    "            json.dump(data_dict, jf, indent=4)\n",
    "            print(f'Successfully saved preprocessed data to {json_path}!')\n",
    "            file_count = len(data_dict['files'])\n",
    "            print(f'{file_count} audio samples were processed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = dataset_path + 'preprocessed_data.json'\n",
    "preprocess_audio_dataset(audio_path, json_path=json_path, mfcc_count=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
