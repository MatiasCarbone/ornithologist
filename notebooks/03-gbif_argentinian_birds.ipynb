{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File management\n",
    "import os\n",
    "import glob\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Audio processing\n",
    "import librosa as li\n",
    "from scipy import signal\n",
    "import soundfile as sf\n",
    "\n",
    "# Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "# Other\n",
    "import time\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User defined constants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target sample rate for resampling audio files\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# Target length for audio segments (in seconds)\n",
    "SAMPLE_LENGTH = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download argentinian birds dataset from Xeno-Canto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Xeno-Canto API and save response as JSON file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = \"..\\\\datasets\\\\xeno-canto_argentina\\\\\"\n",
    "\n",
    "# Query variables\n",
    "country = \"argentina\"\n",
    "group = \"birds\"\n",
    "length = \"2-60\"\n",
    "since = \"2010-01-01\"\n",
    "q = \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://xeno-canto.org/api/2/recordings?query=\"\n",
    "params = f\"cnt:{country}+grp:{group}+len:{length}+since:{since}+q:{q}\"\n",
    "\n",
    "response = requests.get(url + params)\n",
    "\n",
    "print(f\"• Query result: status-code {response.status_code}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "    n_rec = data[\"numRecordings\"]\n",
    "    pages = data[\"numPages\"]\n",
    "    print(f\"• Found {n_rec} recordings in {pages} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write json files for all pages\n",
    "for p in range(1, pages + 1):\n",
    "    response = requests.get(url + params + f\"&page={p}\")\n",
    "    data = response.json()\n",
    "\n",
    "    filename = f\"query_{str(p)}.json\"\n",
    "    with open(dataset_location + filename, \"w\") as file:\n",
    "        json.dump(data, file, sort_keys=True, indent=4)\n",
    "        print(f\"• Saved page {p} as {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download files to dataset audio folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each category will be downloaded to their corresponding subfolder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create audio folder inside dataset.\n",
    "audio_location = dataset_location + \"unprocessed_audio_files\\\\\"\n",
    "try:\n",
    "    os.mkdir(audio_location)\n",
    "    print(f\"Created {audio_location}\")\n",
    "except:\n",
    "    print(\"Folder already existed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(dataset_location):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(dataset_location + file) as f:\n",
    "            data = json.load(f)\n",
    "            recordings = data[\"recordings\"]\n",
    "            print(f\"Downloading files from {file}...\")\n",
    "\n",
    "            for r in recordings:\n",
    "                # Get metadata from json\n",
    "                id = r[\"id\"]\n",
    "                # Optional, if coloquial name is preferred\n",
    "                english_name = r[\"en\"]\n",
    "                scientific_name = f\"{r['gen']} {r['sp']}\"\n",
    "                download = r[\"file\"]\n",
    "                ext = \".\" + r[\"file-name\"].split(\".\")[-1]\n",
    "\n",
    "                # Create subfolder if not exists\n",
    "                subfolder = scientific_name + \"\\\\\"\n",
    "                try:\n",
    "                    os.mkdir(audio_location + subfolder)\n",
    "                except:  # noqa: E722\n",
    "                    pass\n",
    "\n",
    "                # Download file\n",
    "                with open(\n",
    "                    audio_location + subfolder + scientific_name + \"_\" + id + ext, \"wb\"\n",
    "                ) as out_file:\n",
    "                    content = requests.get(download, stream=True).content\n",
    "                    out_file.write(content)\n",
    "\n",
    "                # Wait required time between recordings (randomized)\n",
    "                time.sleep(random.uniform(1.01, 1.2))\n",
    "\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Wait some time between json pages (randomized)\n",
    "    time.sleep(random.uniform(1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess audio files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = signal.butter(order, normal_cutoff, btype=\"high\", analog=False)\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def apply_butter_highpass(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_silence(signal, thresh=18, hop=2048, plot=False):\n",
    "    splits = li.effects.split(\n",
    "        y=signal, top_db=thresh, frame_length=(hop * 2), hop_length=hop\n",
    "    )\n",
    "\n",
    "    # For fine-tuning purposes\n",
    "    if plot:\n",
    "        peak = np.max(signal)\n",
    "        plt.subplots(figsize=(12, 4))\n",
    "        plt.plot(signal)\n",
    "        plt.vlines(splits, ymin=-peak, ymax=peak, color=\"red\")\n",
    "        plt.show()\n",
    "\n",
    "    stripped_audio = []\n",
    "\n",
    "    for s in splits:\n",
    "        split = signal[s[0] : s[1]]\n",
    "        stripped_audio.extend(split)\n",
    "\n",
    "    return np.asarray(stripped_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target length must be in seconds\n",
    "def split_audio(signal, target_length, samplerate, plot=False):\n",
    "\n",
    "    duration = li.get_duration(y=signal, sr=samplerate)\n",
    "\n",
    "    n_segments = np.ceil(duration / target_length)\n",
    "\n",
    "    audio_segments = []\n",
    "\n",
    "    for n in range(int(n_segments)):\n",
    "\n",
    "        s = signal[\n",
    "            samplerate * n * target_length : samplerate * (n + 1) * target_length\n",
    "        ]\n",
    "\n",
    "        if len(s) < target_length * samplerate:\n",
    "\n",
    "            s = np.pad(s, (0, target_length * samplerate - len(s)), \"constant\")\n",
    "\n",
    "        audio_segments.append(s)\n",
    "\n",
    "        if plot:\n",
    "\n",
    "            plt.plot(s, alpha=1 / n_segments)\n",
    "\n",
    "    if plot:\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return audio_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load each file from all unprocessed audio subfolders.\n",
    "- Make an empty copy of the folder in the processed directory.\n",
    "- Apply high filter, strip noise sections and split into constant length segments.\n",
    "- Save shorter segments as new WAV files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_location = dataset_location + \"processed_audio_files\\\\\"\n",
    "subfolders = os.listdir(audio_location)\n",
    "\n",
    "# Make processed audio folder\n",
    "try:\n",
    "    os.mkdir(processed_location)\n",
    "except:  # noqa: E722\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional list to filter species\n",
    "wanted_species = [\n",
    "    \"Anas bahamensis\",\n",
    "    \"Bubulcus ibis\",\n",
    "    \"Caracara plancus\",\n",
    "    \"Coereba flaveola\",\n",
    "    \"Columba livia\",\n",
    "    \"Columbina talpacoti\",\n",
    "    \"Coragyps atratus\",\n",
    "    \"Crotophaga ani\",\n",
    "    \"Egretta thula\",\n",
    "    \"Furnarius rufus\",\n",
    "    \"Haematopus palliatus\",\n",
    "    \"Jacana jacana\",\n",
    "    \"Larus dominicanus\",\n",
    "    \"Mimus saturninus\",\n",
    "    \"Myiopsitta monachus\",\n",
    "    \"Nannopterum brasilianum\",\n",
    "    \"Paroaria coronata\",\n",
    "    \"Pitangus sulphuratus\",\n",
    "    \"Pyrocephalus obscurus\",\n",
    "    \"Ramphastos toco\",\n",
    "    \"Sicalis flaveola\",\n",
    "    \"Turdus rufiventris\",\n",
    "    \"Tyrannus melancholicus\"\n",
    "    \"Vanellus chilensis\",\n",
    "    \"Zenaida auriculata\",\n",
    "    \"Zonotrichia capensis\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in subfolders:\n",
    "    # Optional filter for keeping only desired species\n",
    "    if sub not in wanted_species:\n",
    "        continue\n",
    "\n",
    "    files = os.listdir(audio_location + sub)\n",
    "\n",
    "    # Create same subfolder in processed folder\n",
    "    p_subfolder = processed_location + sub + \"\\\\\"\n",
    "\n",
    "    try:\n",
    "        os.mkdir(p_subfolder)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for f in files:\n",
    "        # Load audio file from unprocessed folder\n",
    "        file_path = audio_location + sub + \"\\\\\" + f\n",
    "        y, sr = li.load(file_path, sr=SAMPLE_RATE, mono=True, res_type=\"soxr_lq\")\n",
    "\n",
    "        # Apply high-pass filter\n",
    "        y = apply_butter_highpass(data=y, cutoff=1000, fs=SAMPLE_RATE, order=6)\n",
    "\n",
    "        # Delete silent sections\n",
    "        y = remove_silence(y, plot=False)\n",
    "\n",
    "        # Split into segments of desired length\n",
    "        audio_segments = split_audio(\n",
    "            y, target_length=SAMPLE_LENGTH, samplerate=SAMPLE_RATE\n",
    "        )\n",
    "\n",
    "        # Iterate through splitted audio segments and save each one as a separate .wav file\n",
    "        for i, segment in enumerate(audio_segments):\n",
    "            new_filename = f.split(\".\")[0] + \"_\" + str(i) + \".wav\"\n",
    "            sf.write(p_subfolder + new_filename, segment, SAMPLE_RATE, subtype=\"PCM_16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow dataset generation, processing and data pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map functions definitions\n",
    "\n",
    "Used for loading and processing audio data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio file\n",
    "def load_audio(file_path):\n",
    "    # Load encoded audio file\n",
    "    file = tf.io.read_file(file_path)\n",
    "\n",
    "    # Decode wav file\n",
    "    audio, sr = tf.audio.decode_wav(file, desired_channels=1)\n",
    "\n",
    "    # Remove channels dimension\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    sr = tf.cast(sr, dtype=tf.int64)\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert audio signal to spectrogram\n",
    "def process_audio(file_path, label):\n",
    "    # Load audio\n",
    "    audio = load_audio(file_path)\n",
    "\n",
    "    # Compute spectrogram\n",
    "    spectrogram = tf.signal.stft(audio, frame_length=320, frame_step=32)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "\n",
    "    return spectrogram, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode string labels as int. Returns list of encoded labels and labels dictionary.\n",
    "def encode_labels(labels_list):\n",
    "    labels_dict = {}\n",
    "    encoded_labels = []\n",
    "\n",
    "    for i, label in enumerate(set(labels_list)):\n",
    "        labels_dict[label] = i\n",
    "\n",
    "    for label in labels_list:\n",
    "        encoded_labels.append(labels_dict[label])\n",
    "\n",
    "    return encoded_labels, labels_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create TF Data Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_location = \"..\\\\datasets\\\\xeno-canto_argentina\\\\processed_audio_files\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels as ints for using in ML model\n",
    "regex = \"\\\\*\\\\*.wav\"\n",
    "audio_filepaths = glob.glob(processed_location + regex)\n",
    "\n",
    "str_labels = list(map(lambda x: x.split(\"\\\\\")[-2], audio_filepaths))\n",
    "encoded_labels, labels_dict = encode_labels(str_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_ds = tf.data.Dataset.list_files(processed_location + \"*\\\\*.wav\", shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "labels = tf.data.Dataset.from_tensor_slices(encoded_labels)\n",
    "\n",
    "\n",
    "\n",
    "# Zip together audio dataset with labels dataset\n",
    "\n",
    "\n",
    "\n",
    "ds = tf.data.Dataset.zip((audio_ds, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test and visualize load and process audio functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath, label = ds.shuffle(1000).as_numpy_iterator().next()\n",
    "spectrogram, label = process_audio(filepath, label)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(tf.transpose(spectrogram)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build data pipeline and split into train/test subsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and batch dataset\n",
    "ds = ds.map(process_audio)\n",
    "ds = ds.cache()\n",
    "ds = ds.shuffle(buffer_size=10000)\n",
    "ds = ds.batch(16)\n",
    "ds = ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "SPLIT = 0.7\n",
    "\n",
    "train_split = int(len(ds) * SPLIT)\n",
    "train_ds = ds.take(train_split)\n",
    "test_ds = ds.skip(train_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train CNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_class_count = len(labels_dict)\n",
    "\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.InputLayer(input_shape=(1491, 257, 1)),\n",
    "        tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=unique_class_count, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "# Training checkpoints\n",
    "checkpoint_filepath = \"../models/checkpoints/\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=test_ds,\n",
    "    callbacks=[model_checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting model accuracy and loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label=\"Training Accuracy\")\n",
    "plt.plot(epochs_range, val_acc, label=\"Validation Accuracy\")\n",
    "plt.grid()\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label=\"Training Loss\")\n",
    "plt.plot(epochs_range, val_loss, label=\"Validation Loss\")\n",
    "plt.grid()\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "\n",
    "figure = plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and load Keras models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = str(input(\"Enter a title for the saved model...\"))\n",
    "model_path = f\"../models/{model_name}/\"\n",
    "\n",
    "if model_name:\n",
    "    try:\n",
    "        os.mkdir(model_path)\n",
    "        model.save(model_path + f\"{model_name}.keras\")\n",
    "        print(f\"Model successfully saved to {model_path}{model_name}.\")\n",
    "\n",
    "        # Remember to execute plotting cell first\n",
    "        figure.savefig(model_path + f\"{model_name}.png\")\n",
    "    except:\n",
    "        raise (\n",
    "            \"ERROR: Model name is already in use. Pick another name and try again.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = str(input(\"Type model name...\"))\n",
    "\n",
    "if model_name:\n",
    "    model_path = f\"../models/{model_name}/\"\n",
    "\n",
    "# model = tf.keras.models.load_model(model_path + f\"{model_name}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing and prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mp3 stereo file for making predictions\n",
    "def load_mp3_for_prediction(filename):\n",
    "    # Load file\n",
    "    iot = tfio.audio.AudioIOTensor(filename)\n",
    "\n",
    "    # Convert to tensor and combine channels to mono\n",
    "    tensor = iot.to_tensor()\n",
    "    tensor = tf.math.reduce_sum(tensor, axis=1) / 2\n",
    "\n",
    "    # Get sample rate\n",
    "    sample_rate = iot.rate\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "\n",
    "    # Resample to target sample rate\n",
    "    wav = tfio.audio.resample(tensor,\n",
    "                              rate_in=sample_rate,\n",
    "                              rate_out=SAMPLE_RATE)\n",
    "    return wav\n",
    "\n",
    "\n",
    "# Convert slices of audio into spectrograms\n",
    "\n",
    "\n",
    "def process_slices(sample, index):\n",
    "    sample = sample[0]\n",
    "    zero_padding = tf.zeros([48000] - tf.shape(sample), dtype=tf.float32)\n",
    "    wav = tf.concat([zero_padding, sample], 0)\n",
    "    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "    return spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_audio_path = \"..\\\\datasets\\\\xeno-canto_argentina\\\\Caracara plancus.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio file for prediction\n",
    "audio = load_mp3_for_prediction(prediction_audio_path)\n",
    "\n",
    "# Get windows of audio\n",
    "audio_slices = tf.keras.utils.timeseries_dataset_from_array(\n",
    "    audio,\n",
    "    audio,\n",
    "    sequence_length=SAMPLE_RATE * SAMPLE_LENGTH,\n",
    "    sequence_stride=SAMPLE_RATE * SAMPLE_LENGTH,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "# Convert audio slices to spectrograms\n",
    "samples, index = audio_slices.as_numpy_iterator().next()\n",
    "audio_slices = audio_slices.map(process_slices)\n",
    "audio_slices = audio_slices.batch(32)\n",
    "\n",
    "# Make predictions for each audio slice\n",
    "predictions = model.predict(audio_slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in predictions:\n",
    "    label = p.argmax()\n",
    "    label_str = list(labels_dict.keys())[label]\n",
    "    probability = np.round(p.max() * 100)\n",
    "\n",
    "    print(label, probability)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
