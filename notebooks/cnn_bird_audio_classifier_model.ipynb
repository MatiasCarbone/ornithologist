{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import opendatasets as od\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import librosa as li\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset download and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation function declarations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate butterworth highpass coefficients\n",
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "\n",
    "\n",
    "# Apply filter to signal\n",
    "def apply_butter_highpass(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove sections of silence or low intensity signal\n",
    "def remove_silence(signal, thresh=18, hop=2048, plot=False):\n",
    "    splits = li.effects.split(\n",
    "        y=signal, top_db=thresh, frame_length=(hop * 2), hop_length=hop\n",
    "    )\n",
    "    if plot:\n",
    "        peak = np.max(signal)\n",
    "        plt.subplots(figsize=(12, 4))\n",
    "        plt.plot(signal)\n",
    "        plt.vlines(splits, ymin=-peak, ymax=peak, color='red')\n",
    "        plt.show()\n",
    "\n",
    "    stripped_audio = []\n",
    "\n",
    "    for s in splits:\n",
    "        split = signal[s[0]: s[1]]\n",
    "        stripped_audio.extend(split)\n",
    "\n",
    "    return np.asarray(stripped_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split audio into segments of desired length\n",
    "def split_audio(signal, target_length, samplerate, plot=False):\n",
    "    duration = li.get_duration(y=signal, sr=samplerate)\n",
    "    n_segments = np.ceil(duration / target_length)\n",
    "    audio_segments = []\n",
    "\n",
    "    for n in range(int(n_segments)):\n",
    "        s = signal[\n",
    "            samplerate * n * target_length: samplerate * (n + 1) * target_length\n",
    "        ]\n",
    "\n",
    "        if len(s) < target_length * samplerate:\n",
    "            s = np.pad(s, (0, target_length * samplerate - len(s)), 'constant')\n",
    "\n",
    "        audio_segments.append(s)\n",
    "\n",
    "        if plot:\n",
    "            plt.plot(s, alpha=1 / n_segments)\n",
    "\n",
    "    if plot:\n",
    "        plt.show()\n",
    "\n",
    "    return audio_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all signal processing functions to audio and return segments\n",
    "def generate_preprocessed_samples(path, sr, length, hp=700):\n",
    "\n",
    "    y, sr = li.load(path, sr=sr, mono=True)  # Load audio file\n",
    "\n",
    "    y = apply_butter_highpass(\n",
    "        data=y, cutoff=hp, fs=sr, order=5\n",
    "    )  # Apply high-pass filter\n",
    "\n",
    "    # Delete silent sections\n",
    "    y = remove_silence(y, thresh=18, hop=2048, plot=False)\n",
    "\n",
    "    audio_segments = split_audio(\n",
    "        y, target_length=length, samplerate=sr\n",
    "    )  # Split into segments of desired length\n",
    "\n",
    "    return audio_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading british-birdsong-dataset.zip to ..\\datasets\\british-birdsong-dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633M/633M [00:23<00:00, 27.8MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_url = (\n",
    "    'https://www.kaggle.com/datasets/rtatman/british-birdsong-dataset?resource=download'\n",
    ")\n",
    "dowload_path = '..\\\\datasets\\\\'\n",
    "\n",
    "od.download(dataset_url, data_dir=dowload_path)\n",
    "\n",
    "dataset_path = dowload_path + 'british-birdsong-dataset\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse metadata csv\n",
    "metadata = pd.read_csv(\n",
    "    dataset_path + 'birdsong_metadata.csv',\n",
    "    usecols=['file_id', 'genus', 'species', 'english_cname'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio data samples preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User defined constants for audio preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target sample rate for resampling audio files\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# Target length for audio segments (in seconds)\n",
    "SAMPLE_LENGTH = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all audio files from directory, apply filter, remove silent sections and split into segments. Save each segment to disk, with filename according to scientific bird name corresponding to dataset metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new folder: ..\\datasets\\british-birdsong-dataset\\audio\\\n",
      "Finished dataset preprocessing!\n"
     ]
    }
   ],
   "source": [
    "old_path = dataset_path + 'songs\\\\songs\\\\'\n",
    "audio_path = dataset_path + 'audio\\\\'\n",
    "\n",
    "try:\n",
    "    os.mkdir(audio_path)\n",
    "    print('Created new folder: ' + audio_path)\n",
    "except:\n",
    "    print(audio_path + ' already exists\\n')\n",
    "\n",
    "for file in os.listdir(old_path):\n",
    "    id = file.lstrip('xc').rstrip('.flac')\n",
    "    data = metadata.loc[metadata['file_id'] == int(id)]\n",
    "\n",
    "    name = data['genus'].item() + '_' + data['species'].item()\n",
    "    subfolder = audio_path + name + '\\\\'\n",
    "    try:\n",
    "        os.mkdir(subfolder)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Preprocess audio and get segments of desired length\n",
    "    audio_segments = generate_preprocessed_samples(\n",
    "        old_path + file, sr=SAMPLE_RATE, length=SAMPLE_LENGTH\n",
    "    )\n",
    "\n",
    "    # Iterate through splitted audio segments and save each one as a separate flac file\n",
    "    for i, segment in enumerate(audio_segments):\n",
    "        new_filename = f'{subfolder}{name}_{id}_{i}.flac'\n",
    "        sf.write(new_filename, segment, SAMPLE_RATE,\n",
    "                 format='flac', subtype='PCM_16')\n",
    "\n",
    "shutil.rmtree(dataset_path + '\\\\songs')\n",
    "\n",
    "print('Finished generating audio samples!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset path\n",
    "dataset_path = '..\\\\datasets\\\\british-birdsong-dataset\\\\'\n",
    "# Get audio path\n",
    "audio_path = dataset_path + \"audio\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read samples from disk and store file path, label, labels mapping and MFCC data into JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio_dataset(data_path, json_path=None, mfcc_count=13, hop=512, fft_len=2048):\n",
    "    data_dict = {\n",
    "        'label_map': [],\n",
    "        'encoded_labels': [],\n",
    "        'mfccs': [],\n",
    "        'files': [],\n",
    "    }\n",
    "\n",
    "    for i, (path, _, files) in enumerate(os.walk(data_path)):\n",
    "        if path == data_path:  # Ignore parent folder\n",
    "            continue\n",
    "\n",
    "        # Add unique labels to label_map list\n",
    "        label = path.split('\\\\')[-1]\n",
    "        if label not in data_dict['label_map']:\n",
    "            data_dict['label_map'].append(label)\n",
    "\n",
    "        for f in files:\n",
    "            # Add encoded label to encoded_labels list\n",
    "            index = data_dict['label_map'].index(label)\n",
    "            data_dict['encoded_labels'].append(index)\n",
    "\n",
    "            # Add file path to files list\n",
    "            data_dict['files'].append(os.path.join(path, f))\n",
    "\n",
    "            # Load audio and add MFCCs to list\n",
    "            y, sr = li.load(os.path.join(path, f), sr=None, mono=True)\n",
    "            mfccs = li.feature.mfcc(y=y, sr=sr, n_mfcc=mfcc_count, hop_length=hop, n_fft=fft_len)\n",
    "\n",
    "            # Cast np.array to list is needed to save as JSON file\n",
    "            data_dict['mfccs'].append(mfccs.transpose().tolist())\n",
    "\n",
    "    # Store in json file\n",
    "    if json_path:\n",
    "        with open(json_path, 'w') as jf:\n",
    "            json.dump(data_dict, jf, indent=4)\n",
    "            print(f'Successfully saved preprocessed data to {json_path}!')\n",
    "    # return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved preprocessed data to ..\\datasets\\british-birdsong-dataset\\preprocessed_data.json!\n"
     ]
    }
   ],
   "source": [
    "json_path = dataset_path + 'preprocessed_data.json'\n",
    "preprocess_audio_dataset(audio_path, json_path=json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read JSON data and generate train/test/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_splits(data_path, test_size=0.2, val_size=0.2):\n",
    "    # Read JSON and extract X and Y values\n",
    "    with open(data_path, 'r') as jf:\n",
    "        data = json.load(jf)\n",
    "    x = np.array(data['mfccs'])\n",
    "    y = np.array(data['encoded_labels'])\n",
    "    label_map = data['label_map']\n",
    "\n",
    "    # Generate splits\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=val_size)\n",
    "\n",
    "    # Add third dimension to MFCC arrays\n",
    "    x_train = x_train[..., np.newaxis]\n",
    "    x_test = x_test[..., np.newaxis]\n",
    "    x_val = x_val[..., np.newaxis]\n",
    "\n",
    "    return x_train, x_test, x_val, y_train, y_test, y_val, label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '..\\\\datasets\\\\british-birdsong-dataset\\\\preprocessed_data.json'\n",
    "\n",
    "x_train, x_test, x_val, y_train, y_test, y_val, label_map = generate_splits(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_21 (Conv2D)          (None, 92, 11, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 92, 11, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 46, 6, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 44, 4, 32)         18464     \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 44, 4, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 22, 2, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 21, 1, 32)         4128      \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 21, 1, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 11, 1, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 352)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                22592     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 88)                5720      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,056\n",
      "Trainable params: 51,800\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (x_train.shape[1], x_train.shape[2], x_train.shape[3])\n",
    "class_count = len(label_map)\n",
    "\n",
    "model = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer(input_shape=(94, 13, 1)),\n",
    "        # Convolution layer 1\n",
    "        keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=(3, 3),\n",
    "            activation='relu',\n",
    "            kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same'),\n",
    "        # Convolution layer 2\n",
    "        keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=(3, 3),\n",
    "            activation='relu',\n",
    "            kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same'),\n",
    "        # Convolution layer 3\n",
    "        keras.layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=(2, 2),\n",
    "            activation='relu',\n",
    "            kernel_regularizer=keras.regularizers.l2(0.001),\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='same'),\n",
    "        # Dense layer\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(units=64, activation='relu'),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        # Classification layer\n",
    "        keras.layers.Dense(units=class_count, activation='softmax'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
